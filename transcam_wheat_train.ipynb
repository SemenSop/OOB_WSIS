{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IpMt144pm_R_",
        "outputId": "2d38fb58-08d1-478d-bd8a-798e8e18cbea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/root\n",
            "Mounted at /content/drive/\n",
            "/content/drive/MyDrive/Colab Notebooks/WSSS\n",
            "\u001b[0m\u001b[01;34mAdelaiDet\u001b[0m/                   res38_cls.pth\n",
            "ADL_notok.ipynb              \u001b[01;34mSEAM\u001b[0m/\n",
            "\u001b[01;34mAuxSegNet\u001b[0m/                   \u001b[01;34mSESS\u001b[0m/\n",
            "\u001b[01;34mBANA\u001b[0m/                        \u001b[01;34msrc\u001b[0m/\n",
            "\u001b[01;34mBBAM\u001b[0m/                        test_img2.jpg\n",
            "BBAM_notok.ipynb             test_img.jpg\n",
            "\u001b[01;34mBESTIE\u001b[0m/                      \u001b[01;34mTokenCut\u001b[0m/\n",
            "BoxInst_notok.ipynb          TokenCut_ok.ipynb\n",
            "\u001b[01;34mCCAM\u001b[0m/                        \u001b[01;34mtranscam\u001b[0m/\n",
            "CCAM_ok.ipynb                transcam_6485.pth\n",
            "\u001b[01;34mCOCO2YOLO\u001b[0m/                   transcam_deeplab.ipynb\n",
            "Conformer_small_patch16.pth  transcam_instance_ok.ipynb\n",
            "\u001b[01;34mdata_voc\u001b[0m/                    transcam_instance_v2_ok.ipynb\n",
            "\u001b[01;34mDiscoBox\u001b[0m/                    transcam_instance_v3_ok.ipynb\n",
            "DiscoBox_notok.ipynb         transcam_masks_ok.ipynb\n",
            "\u001b[01;34mexternals\u001b[0m/                   transcam_ok.ipynb\n",
            "instances_val2017.json       transcam_train.ipynb\n",
            "instance_train_ok.ipynb      transcam_wheat_ok.ipynb\n",
            "\u001b[01;34misim\u001b[0m/                        transcam_wheat_train.ipynb\n",
            "isim_notok.ipynb             \u001b[01;34mwheat\u001b[0m/\n",
            "\u001b[01;34mLOST\u001b[0m/                        wheat_for_transcam_notok.ipynb\n",
            "LOST_ok.ipynb                yolo_test.ipynb\n",
            "midas_ok.ipynb               \u001b[01;34myolov5\u001b[0m/\n",
            "my_test_v2.ipynb             yolov5.ipynb\n",
            "\u001b[01;34mPuzzleCAM\u001b[0m/                   \u001b[01;34myolov7-segmentation\u001b[0m/\n"
          ]
        }
      ],
      "source": [
        "%cd\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "%cd /content/drive/MyDrive/Colab Notebooks/WSSS\n",
        "%ls"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd transcam\n",
        "!pip install -r requirements.txt\n",
        "%ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9gMVhnbMB-zl",
        "outputId": "bda7ba26-76fc-490f-c6cc-c1ca6db2f8e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks/WSSS/transcam\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/lucasb-eyer/pydensecrf.git (from -r requirements.txt (line 12))\n",
            "  Cloning https://github.com/lucasb-eyer/pydensecrf.git to /tmp/pip-req-build-9o_y862k\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/lucasb-eyer/pydensecrf.git /tmp/pip-req-build-9o_y862k\n",
            "  Resolved https://github.com/lucasb-eyer/pydensecrf.git to commit 0d53acbcf5123d4c88040fe68fbb9805fc5b2fb9\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting torch==1.10.1\n",
            "  Downloading torch-1.10.1-cp38-cp38-manylinux1_x86_64.whl (881.9 MB)\n",
            "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m881.2/881.9 MB\u001b[0m \u001b[31m188.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0mtcmalloc: large alloc 1102397440 bytes == 0x38756000 @  0x7f2959e43615 0x5d6f4c 0x51edd1 0x51ef5b 0x4f750a 0x4997a2 0x55cd91 0x5d8941 0x4997a2 0x55cd91 0x5d8941 0x4997a2 0x55cd91 0x5d8941 0x4997a2 0x55cd91 0x5d8941 0x4997a2 0x55cd91 0x5d8941 0x4997a2 0x5d8868 0x4997a2 0x55cd91 0x5d8941 0x49abe4 0x55cd91 0x5d8941 0x4997a2 0x55cd91 0x5d8941\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m881.9/881.9 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchvision==0.11.2\n",
            "  Downloading torchvision-0.11.2-cp38-cp38-manylinux1_x86_64.whl (23.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.3/23.3 MB\u001b[0m \u001b[31m69.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting opencv-python==4.5.5.62\n",
            "  Downloading opencv_python-4.5.5.62-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (60.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.4/60.4 MB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy==1.7.3 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 4)) (1.7.3)\n",
            "Collecting tensorboard==2.7.0\n",
            "  Downloading tensorboard-2.7.0-py3-none-any.whl (5.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m48.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting timm==0.4.12\n",
            "  Downloading timm-0.4.12-py3-none-any.whl (376 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m377.0/377.0 KB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas==1.3.5 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 7)) (1.3.5)\n",
            "Collecting scikit-image==0.19.1\n",
            "  Downloading scikit_image-0.19.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m60.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tqdm==4.62.3\n",
            "  Downloading tqdm-4.62.3-py2.py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.2/76.2 KB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting mxnet==1.9.0\n",
            "  Downloading mxnet-1.9.0-py3-none-manylinux2014_x86_64.whl (47.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.3/47.3 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting matplotlib==3.5.1\n",
            "  Downloading matplotlib-3.5.1-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (11.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m88.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch==1.10.1->-r requirements.txt (line 1)) (4.4.0)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision==0.11.2->-r requirements.txt (line 2)) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torchvision==0.11.2->-r requirements.txt (line 2)) (1.21.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard==2.7.0->-r requirements.txt (line 5)) (2.25.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard==2.7.0->-r requirements.txt (line 5)) (57.4.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard==2.7.0->-r requirements.txt (line 5)) (1.8.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.8/dist-packages (from tensorboard==2.7.0->-r requirements.txt (line 5)) (1.3.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard==2.7.0->-r requirements.txt (line 5)) (0.6.1)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard==2.7.0->-r requirements.txt (line 5)) (1.51.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.8/dist-packages (from tensorboard==2.7.0->-r requirements.txt (line 5)) (0.38.4)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.8/dist-packages (from tensorboard==2.7.0->-r requirements.txt (line 5)) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard==2.7.0->-r requirements.txt (line 5)) (3.4.1)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard==2.7.0->-r requirements.txt (line 5)) (3.19.6)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard==2.7.0->-r requirements.txt (line 5)) (2.15.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard==2.7.0->-r requirements.txt (line 5)) (0.4.6)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas==1.3.5->-r requirements.txt (line 7)) (2022.7)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas==1.3.5->-r requirements.txt (line 7)) (2.8.2)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from scikit-image==0.19.1->-r requirements.txt (line 8)) (1.4.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from scikit-image==0.19.1->-r requirements.txt (line 8)) (21.3)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.8/dist-packages (from scikit-image==0.19.1->-r requirements.txt (line 8)) (2.8.8)\n",
            "Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.8/dist-packages (from scikit-image==0.19.1->-r requirements.txt (line 8)) (2.9.0)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.8/dist-packages (from scikit-image==0.19.1->-r requirements.txt (line 8)) (2022.10.10)\n",
            "Collecting graphviz<0.9.0,>=0.8.1\n",
            "  Downloading graphviz-0.8.4-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib==3.5.1->-r requirements.txt (line 11)) (1.4.4)\n",
            "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib==3.5.1->-r requirements.txt (line 11)) (3.0.9)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib==3.5.1->-r requirements.txt (line 11)) (0.11.0)\n",
            "Collecting fonttools>=4.22.0\n",
            "  Downloading fonttools-4.38.0-py3-none-any.whl (965 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m965.4/965.4 KB\u001b[0m \u001b[31m37.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard==2.7.0->-r requirements.txt (line 5)) (4.9)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard==2.7.0->-r requirements.txt (line 5)) (1.15.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard==2.7.0->-r requirements.txt (line 5)) (0.2.8)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard==2.7.0->-r requirements.txt (line 5)) (5.2.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard==2.7.0->-r requirements.txt (line 5)) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard==2.7.0->-r requirements.txt (line 5)) (6.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard==2.7.0->-r requirements.txt (line 5)) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard==2.7.0->-r requirements.txt (line 5)) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard==2.7.0->-r requirements.txt (line 5)) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard==2.7.0->-r requirements.txt (line 5)) (2.10)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard==2.7.0->-r requirements.txt (line 5)) (3.11.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard==2.7.0->-r requirements.txt (line 5)) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard==2.7.0->-r requirements.txt (line 5)) (3.2.2)\n",
            "Building wheels for collected packages: pydensecrf\n",
            "  Building wheel for pydensecrf (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pydensecrf: filename=pydensecrf-1.0rc2-cp38-cp38-linux_x86_64.whl size=2845227 sha256=479f0e2aedb2c00f89a96addc8dcb2fc5bfa2e9b1e3262def28095572457b732\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-zmg05k9v/wheels/45/56/7a/826b4f5cd8459926ff5996ba9994fc36672487b6d2fa50d3d4\n",
            "Successfully built pydensecrf\n",
            "Installing collected packages: pydensecrf, tqdm, torch, opencv-python, graphviz, fonttools, torchvision, scikit-image, mxnet, matplotlib, timm, tensorboard\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.64.1\n",
            "    Uninstalling tqdm-4.64.1:\n",
            "      Successfully uninstalled tqdm-4.64.1\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.13.0+cu116\n",
            "    Uninstalling torch-1.13.0+cu116:\n",
            "      Successfully uninstalled torch-1.13.0+cu116\n",
            "  Attempting uninstall: opencv-python\n",
            "    Found existing installation: opencv-python 4.6.0.66\n",
            "    Uninstalling opencv-python-4.6.0.66:\n",
            "      Successfully uninstalled opencv-python-4.6.0.66\n",
            "  Attempting uninstall: graphviz\n",
            "    Found existing installation: graphviz 0.10.1\n",
            "    Uninstalling graphviz-0.10.1:\n",
            "      Successfully uninstalled graphviz-0.10.1\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.14.0+cu116\n",
            "    Uninstalling torchvision-0.14.0+cu116:\n",
            "      Successfully uninstalled torchvision-0.14.0+cu116\n",
            "  Attempting uninstall: scikit-image\n",
            "    Found existing installation: scikit-image 0.18.3\n",
            "    Uninstalling scikit-image-0.18.3:\n",
            "      Successfully uninstalled scikit-image-0.18.3\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.2.2\n",
            "    Uninstalling matplotlib-3.2.2:\n",
            "      Successfully uninstalled matplotlib-3.2.2\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.9.1\n",
            "    Uninstalling tensorboard-2.9.1:\n",
            "      Successfully uninstalled tensorboard-2.9.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchtext 0.14.0 requires torch==1.13.0, but you have torch 1.10.1 which is incompatible.\n",
            "torchaudio 0.13.0+cu116 requires torch==1.13.0, but you have torch 1.10.1 which is incompatible.\n",
            "tensorflow 2.9.2 requires tensorboard<2.10,>=2.9, but you have tensorboard 2.7.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed fonttools-4.38.0 graphviz-0.8.4 matplotlib-3.5.1 mxnet-1.9.0 opencv-python-4.5.5.62 pydensecrf-1.0rc2 scikit-image-0.19.1 tensorboard-2.7.0 timm-0.4.12 torch-1.10.1 torchvision-0.11.2 tqdm-4.62.3\n",
            "data.json           requirements.txt     TransCAM_mytest.pth\n",
            "\u001b[0m\u001b[01;34mdeeplab\u001b[0m/            \u001b[01;34mtblog\u001b[0m/               TransCAM_mytest_v2.log\n",
            "evaluation.py       test_deeplab.py      TransCAM_mytest_v2.pth\n",
            "example.png         \u001b[01;34mtest_kaggle\u001b[0m/         TransCAM_mytest_WHEAT_crop.log\n",
            "HUETA00333207f.txt  \u001b[01;34mtool\u001b[0m/                TransCAM_mytest_WHEAT.log\n",
            "HUETA005b0d8bb.txt  train_aff.py         TransCAM_mytest_WHEAT.pth\n",
            "HUETA006a994f7.txt  train_deeplab.py     TransCAM_mytest_WHEAT_v13.log\n",
            "infer_aff.py        \u001b[01;34mtrain_kaggle\u001b[0m/        TransCAM_mytest_WHEAT_v13.pth\n",
            "infer_TransCAM.py   train_TransCAM.py    visualize.ipynb\n",
            "\u001b[01;34mnetwork\u001b[0m/            TransCAM.log         \u001b[01;34mvoc12\u001b[0m/\n",
            "README.md           TransCAM_mytest.log\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import os\n",
        "\n",
        "from torch import optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "import voc12.data\n",
        "from tool import pyutils, imutils\n",
        "import argparse\n",
        "import importlib\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "import PIL.Image\n",
        "import os.path\n",
        "import scipy.misc\n",
        "\n",
        "from glob import glob"
      ],
      "metadata": {
        "id": "4LqEVoAkrRho"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Patch to images\n",
        "train_folder = '../wheat/images/train/'\n",
        "print('Number of files in the train folder', len(os.listdir(train_folder)))"
      ],
      "metadata": {
        "id": "5zJHctLCbXD4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea2ec3da-4c88-41bc-d20f-caa6644e0c7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of files in the train folder 4067\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TransCAM parameters\n",
        "num_workers = 0;\n",
        "lr = 5e-5;\n",
        "wt_dec = 5e-4;\n",
        "batch_size = 2;\n",
        "max_epoches = 20;\n",
        "network = \"network.conformer_CAM\";\n",
        "arch = \"sm\";\n",
        "\n",
        "session_name = \"TransCAM_mytest_WHEAT_v13\"\n",
        "crop_size = 512;\n",
        "weights = '../Conformer_small_patch16.pth'\n",
        "tblog_dir ='./tblog'\n",
        "save_dir ='./';\n"
      ],
      "metadata": {
        "id": "TVFFFHmQoAOG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" Functions \"\"\"\n",
        "# ['hotdog, food, furniture, people, pets]\n",
        "def load_image_label_list_from_npy(img_name_list):\n",
        "  cls_labels_dict = {};\n",
        "\n",
        "  for i in img_name_list:\n",
        "    if ('IMG' in i): \n",
        "      cls_labels_dict[i] = np.array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=np.float32);\n",
        "    else:\n",
        "      cls_labels_dict[i] = np.array([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=np.float32);\n",
        "\n",
        "  return [cls_labels_dict[img_name] for img_name in img_name_list]\n",
        "\n",
        "IMG_FOLDER_NAME = train_folder;\n",
        "def get_img_path(img_name):\n",
        "    return os.path.join(IMG_FOLDER_NAME, img_name + '.jpg')\n",
        "\n",
        "def load_img_name_list(dataset_path):\n",
        "  img_gt_name_list = sorted(glob(os.path.join(dataset_path, \"*.jpg\")));\n",
        "  img_name_list = [img_gt_name.split('/')[-1].split('.')[0] for img_gt_name in img_gt_name_list];\n",
        "  return img_name_list\n",
        "\n",
        "\n",
        "\"\"\" Classes \"\"\"\n",
        "class VOC12ImageDataset(Dataset):\n",
        "\n",
        "    def __init__(self, img_name_list_path, transform=None):\n",
        "        self.img_name_list = load_img_name_list(img_name_list_path) # Image names without .jpg\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_name_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        name = self.img_name_list[idx]\n",
        "\n",
        "        img = PIL.Image.open(get_img_path(name)).convert(\"RGB\") # Open image\n",
        "\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        return name, img\n",
        "        \n",
        "class VOC12ClsDataset(VOC12ImageDataset):\n",
        "\n",
        "    def __init__(self, img_name_list_path, transform=None):\n",
        "        super().__init__(img_name_list_path, transform)\n",
        "        self.label_list = load_image_label_list_from_npy(self.img_name_list) # Massive of the class vectors for each image\n",
        " \n",
        "    def __getitem__(self, idx):\n",
        "        name, img = super().__getitem__(idx)\n",
        "\n",
        "        label = torch.from_numpy(self.label_list[idx])\n",
        "\n",
        "        return name, img, label # Image name; Image; Class vector\n",
        "\n"
      ],
      "metadata": {
        "id": "DTiehbDrmPXo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LnlsGVPRig8x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "pyutils.Logger(session_name + '.log')\n",
        "\n",
        "model = getattr(importlib.import_module(network), 'Net_' + arch)()\n",
        "\n",
        "tblogger = SummaryWriter(tblog_dir)\n",
        "\n",
        "train_dataset = VOC12ClsDataset(train_folder,\n",
        "                                            transform=transforms.Compose([\n",
        "                                                imutils.RandomResizeLong(320, 640),\n",
        "                                                transforms.RandomHorizontalFlip(),\n",
        "                                                transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3,\n",
        "                                                                      hue=0.1),\n",
        "                                                np.asarray,\n",
        "                                                imutils.Normalize(),\n",
        "                                                imutils.RandomCrop(crop_size),\n",
        "                                                imutils.HWC_to_CHW,\n",
        "                                                torch.from_numpy\n",
        "                                            ]))\n",
        "\n",
        "train_data_loader = DataLoader(train_dataset, batch_size=batch_size,\n",
        "                                shuffle=True, num_workers=0, pin_memory=True, drop_last=True)\n",
        "\n",
        "optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=wt_dec, eps=1e-8)\n",
        "\n",
        "checkpoint = torch.load(weights, map_location='cpu')\n",
        "if 'model' in checkpoint.keys():\n",
        "    checkpoint = checkpoint['model']\n",
        "else:\n",
        "    checkpoint = checkpoint\n",
        "model_dict = model.state_dict()\n",
        "for k in ['trans_cls_head.weight', 'trans_cls_head.bias']:\n",
        "    print(f\"Removing key {k} from pretrained checkpoint\")\n",
        "    del checkpoint[k]\n",
        "for k in ['conv_cls_head.weight', 'conv_cls_head.bias']:\n",
        "    print(f\"Removing key {k} from pretrained checkpoint\")\n",
        "    del checkpoint[k]\n",
        "pretrained_dict = {k: v for k, v in checkpoint.items() if k in model_dict}\n",
        "model_dict.update(pretrained_dict)\n",
        "model.load_state_dict(model_dict)\n",
        "\n",
        "model = torch.nn.DataParallel(model).cuda()\n",
        "model.train()\n",
        "\n",
        "avg_meter = pyutils.AverageMeter('loss')\n",
        "\n",
        "TRAIN_LOSS = [];\n",
        "for ep in range(max_epoches):\n",
        "    print('ep num', ep)\n",
        "    for iter, pack in enumerate(train_data_loader):\n",
        "        _, img, label = pack\n",
        "        N, C, H, W = img.size()\n",
        "        bg_score = torch.ones((N, 1))\n",
        "        label = torch.cat((bg_score, label), dim=1)\n",
        "        label = label.cuda(non_blocking=True).unsqueeze(2).unsqueeze(3)\n",
        "\n",
        "        logits_conv, logits_trans, cams = model('transcam', img)\n",
        "\n",
        "        loss = F.multilabel_soft_margin_loss((logits_conv + logits_trans).unsqueeze(2).unsqueeze(3)[:, 1:, :, :], label[:, 1:, :, :])\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        ##########################\n",
        "        # print('epoch: %5d' % ep,\n",
        "        #       'iter: %5d' % iter,\n",
        "        #       'loss: %.4f' % loss, flush=True)\n",
        "        TRAIN_LOSS.append(loss);\n",
        "        ##########################\n",
        "        avg_meter.add({'loss': loss.item()})\n",
        "    else:\n",
        "        print('epoch: %5d' % ep,\n",
        "              'loss: %.4f' % avg_meter.get('loss'), flush=True)\n",
        "        avg_meter.pop()\n",
        "    torch.save(model.module.state_dict(), os.path.join(save_dir, session_name + '.pth'))\n",
        "\n",
        "torch.save(model.module.state_dict(), os.path.join(save_dir, session_name + '.pth'))"
      ],
      "metadata": {
        "id": "fOSg827IoH_I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b58833fc-60d4-410a-a891-6195f8f17720"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Removing key trans_cls_head.weight from pretrained checkpoint\n",
            "Removing key trans_cls_head.bias from pretrained checkpoint\n",
            "Removing key conv_cls_head.weight from pretrained checkpoint\n",
            "Removing key conv_cls_head.bias from pretrained checkpoint\n",
            "ep num 0\n",
            "epoch:     0 loss: 0.0148\n",
            "ep num 1\n",
            "epoch:     1 loss: 0.0024\n",
            "ep num 2\n",
            "epoch:     2 loss: 0.0000\n",
            "ep num 3\n",
            "epoch:     3 loss: 0.0027\n",
            "ep num 4\n",
            "epoch:     4 loss: 0.0025\n",
            "ep num 5\n",
            "epoch:     5 loss: 0.0018\n",
            "ep num 6\n",
            "epoch:     6 loss: 0.0007\n",
            "ep num 7\n",
            "epoch:     7 loss: 0.0027\n",
            "ep num 8\n",
            "epoch:     8 loss: 0.0011\n",
            "ep num 9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "m1lmPnYtoIqY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FHDKYFXB2zQj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VTuzMtZy7vAD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vf8WvAU1oA8x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xb7R61SqBuy-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oe4dMBZ2B55e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "udvZKoDhp10R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "k4EYnl9wp2V0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "p7mQYzSup2yc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Snj_FKwsDJHW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}