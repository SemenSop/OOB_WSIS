{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNgA8Lzm4UcnK+AF5VWrEub"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard","accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IpMt144pm_R_","executionInfo":{"status":"ok","timestamp":1677324218807,"user_tz":-180,"elapsed":25460,"user":{"displayName":"Simon EA45","userId":"04343123245495133347"}},"outputId":"0bb24790-6dd4-4ffb-bda6-e432daae3b8d"},"outputs":[{"output_type":"stream","name":"stdout","text":["/root\n","Mounted at /content/drive/\n","/content/drive/MyDrive/Colab Notebooks/diploma_thesis\n","bbox_ann.csv                 \u001b[0m\u001b[01;34mè«ñêê_2022\u001b[0m/               test_cutler_train.ipynb\n","bbox_ann_train.csv           explore_masks.ipynb       test_cutler_v2.ipynb\n","Conformer_small_patch16.pth  extract_full_masks.ipynb  \u001b[01;34mTransCAM\u001b[0m/\n","\u001b[01;34mCutLER\u001b[0m/                      \u001b[01;34mimagenet\u001b[0m/                 \u001b[01;34multralytics\u001b[0m/\n","\u001b[01;34mdataset\u001b[0m/                     \u001b[01;34mNOTEBOOK_TRANSCAM\u001b[0m/        \u001b[01;34myolo_check\u001b[0m/\n","dataset.ipynb                polygon_masks.py          КодИИ_2022.zip\n","dataset_v2.ipynb             \u001b[01;34m__pycache__\u001b[0m/\n","divide_dataset.ipynb         test_cutler.ipynb\n"]}],"source":["%cd\n","from google.colab import drive\n","drive.mount('/content/drive/')\n","%cd /content/drive/MyDrive/Colab Notebooks/diploma_thesis\n","%ls"]},{"cell_type":"code","source":["!git clone https://github.com/liruiwen/TransCAM.git\n","%cd TransCAM\n","!pip install -r requirements.txt\n","%ls"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9gMVhnbMB-zl","executionInfo":{"status":"ok","timestamp":1677324376381,"user_tz":-180,"elapsed":157579,"user":{"displayName":"Simon EA45","userId":"04343123245495133347"}},"outputId":"557168a0-f7c0-4535-98a0-7b500ab206f8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["fatal: destination path 'TransCAM' already exists and is not an empty directory.\n","/content/drive/MyDrive/Colab Notebooks/diploma_thesis/TransCAM\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting git+https://github.com/lucasb-eyer/pydensecrf.git (from -r requirements.txt (line 12))\n","  Cloning https://github.com/lucasb-eyer/pydensecrf.git to /tmp/pip-req-build-0js6_280\n","  Running command git clone --filter=blob:none --quiet https://github.com/lucasb-eyer/pydensecrf.git /tmp/pip-req-build-0js6_280\n","  Resolved https://github.com/lucasb-eyer/pydensecrf.git to commit 0d53acbcf5123d4c88040fe68fbb9805fc5b2fb9\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Collecting torch==1.10.1\n","  Downloading torch-1.10.1-cp38-cp38-manylinux1_x86_64.whl (881.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m881.9/881.9 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting torchvision==0.11.2\n","  Downloading torchvision-0.11.2-cp38-cp38-manylinux1_x86_64.whl (23.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.3/23.3 MB\u001b[0m \u001b[31m63.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting opencv-python==4.5.5.62\n","  Downloading opencv_python-4.5.5.62-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (60.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.4/60.4 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: scipy==1.7.3 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 4)) (1.7.3)\n","Collecting tensorboard==2.7.0\n","  Downloading tensorboard-2.7.0-py3-none-any.whl (5.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m96.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting timm==0.4.12\n","  Downloading timm-0.4.12-py3-none-any.whl (376 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m377.0/377.0 KB\u001b[0m \u001b[31m40.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pandas==1.3.5 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 7)) (1.3.5)\n","Collecting scikit-image==0.19.1\n","  Downloading scikit_image-0.19.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m90.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting tqdm==4.62.3\n","  Downloading tqdm-4.62.3-py2.py3-none-any.whl (76 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.2/76.2 KB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting mxnet==1.9.0\n","  Downloading mxnet-1.9.0-py3-none-manylinux2014_x86_64.whl (47.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.3/47.3 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting matplotlib==3.5.1\n","  Downloading matplotlib-3.5.1-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (11.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m113.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch==1.10.1->-r requirements.txt (line 1)) (4.5.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torchvision==0.11.2->-r requirements.txt (line 2)) (1.22.4)\n","Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision==0.11.2->-r requirements.txt (line 2)) (7.1.2)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.8/dist-packages (from tensorboard==2.7.0->-r requirements.txt (line 5)) (1.0.1)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.8/dist-packages (from tensorboard==2.7.0->-r requirements.txt (line 5)) (0.38.4)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard==2.7.0->-r requirements.txt (line 5)) (3.4.1)\n","Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard==2.7.0->-r requirements.txt (line 5)) (1.51.1)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard==2.7.0->-r requirements.txt (line 5)) (1.8.1)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard==2.7.0->-r requirements.txt (line 5)) (0.6.1)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard==2.7.0->-r requirements.txt (line 5)) (0.4.6)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard==2.7.0->-r requirements.txt (line 5)) (2.16.1)\n","Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard==2.7.0->-r requirements.txt (line 5)) (3.19.6)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard==2.7.0->-r requirements.txt (line 5)) (57.4.0)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.8/dist-packages (from tensorboard==2.7.0->-r requirements.txt (line 5)) (1.4.0)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard==2.7.0->-r requirements.txt (line 5)) (2.25.1)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas==1.3.5->-r requirements.txt (line 7)) (2.8.2)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas==1.3.5->-r requirements.txt (line 7)) (2022.7.1)\n","Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.8/dist-packages (from scikit-image==0.19.1->-r requirements.txt (line 8)) (2.9.0)\n","Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.8/dist-packages (from scikit-image==0.19.1->-r requirements.txt (line 8)) (2023.2.3)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from scikit-image==0.19.1->-r requirements.txt (line 8)) (23.0)\n","Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.8/dist-packages (from scikit-image==0.19.1->-r requirements.txt (line 8)) (3.0)\n","Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from scikit-image==0.19.1->-r requirements.txt (line 8)) (1.4.1)\n","Collecting graphviz<0.9.0,>=0.8.1\n","  Downloading graphviz-0.8.4-py2.py3-none-any.whl (16 kB)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib==3.5.1->-r requirements.txt (line 11)) (4.38.0)\n","Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib==3.5.1->-r requirements.txt (line 11)) (3.0.9)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib==3.5.1->-r requirements.txt (line 11)) (0.11.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib==3.5.1->-r requirements.txt (line 11)) (1.4.4)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard==2.7.0->-r requirements.txt (line 5)) (0.2.8)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard==2.7.0->-r requirements.txt (line 5)) (1.15.0)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard==2.7.0->-r requirements.txt (line 5)) (5.3.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard==2.7.0->-r requirements.txt (line 5)) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard==2.7.0->-r requirements.txt (line 5)) (1.3.1)\n","Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard==2.7.0->-r requirements.txt (line 5)) (6.0.0)\n","Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard==2.7.0->-r requirements.txt (line 5)) (4.0.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard==2.7.0->-r requirements.txt (line 5)) (2.10)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard==2.7.0->-r requirements.txt (line 5)) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard==2.7.0->-r requirements.txt (line 5)) (2022.12.7)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard==2.7.0->-r requirements.txt (line 5)) (3.14.0)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard==2.7.0->-r requirements.txt (line 5)) (0.4.8)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard==2.7.0->-r requirements.txt (line 5)) (3.2.2)\n","Building wheels for collected packages: pydensecrf\n","  Building wheel for pydensecrf (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pydensecrf: filename=pydensecrf-1.0rc2-cp38-cp38-linux_x86_64.whl size=3693278 sha256=fb83560418fcd622702eeac7cab07003441aa5219949d6f10a3b7686f3dccb57\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-57tdzmi_/wheels/45/56/7a/826b4f5cd8459926ff5996ba9994fc36672487b6d2fa50d3d4\n","Successfully built pydensecrf\n","Installing collected packages: pydensecrf, tqdm, torch, opencv-python, graphviz, torchvision, scikit-image, mxnet, matplotlib, timm, tensorboard\n","  Attempting uninstall: tqdm\n","    Found existing installation: tqdm 4.64.1\n","    Uninstalling tqdm-4.64.1:\n","      Successfully uninstalled tqdm-4.64.1\n","  Attempting uninstall: torch\n","    Found existing installation: torch 1.13.1+cu116\n","    Uninstalling torch-1.13.1+cu116:\n","      Successfully uninstalled torch-1.13.1+cu116\n","  Attempting uninstall: opencv-python\n","    Found existing installation: opencv-python 4.6.0.66\n","    Uninstalling opencv-python-4.6.0.66:\n","      Successfully uninstalled opencv-python-4.6.0.66\n","  Attempting uninstall: graphviz\n","    Found existing installation: graphviz 0.10.1\n","    Uninstalling graphviz-0.10.1:\n","      Successfully uninstalled graphviz-0.10.1\n","  Attempting uninstall: torchvision\n","    Found existing installation: torchvision 0.14.1+cu116\n","    Uninstalling torchvision-0.14.1+cu116:\n","      Successfully uninstalled torchvision-0.14.1+cu116\n","  Attempting uninstall: scikit-image\n","    Found existing installation: scikit-image 0.18.3\n","    Uninstalling scikit-image-0.18.3:\n","      Successfully uninstalled scikit-image-0.18.3\n","  Attempting uninstall: matplotlib\n","    Found existing installation: matplotlib 3.5.3\n","    Uninstalling matplotlib-3.5.3:\n","      Successfully uninstalled matplotlib-3.5.3\n","  Attempting uninstall: tensorboard\n","    Found existing installation: tensorboard 2.11.2\n","    Uninstalling tensorboard-2.11.2:\n","      Successfully uninstalled tensorboard-2.11.2\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torchtext 0.14.1 requires torch==1.13.1, but you have torch 1.10.1 which is incompatible.\n","torchaudio 0.13.1+cu116 requires torch==1.13.1, but you have torch 1.10.1 which is incompatible.\n","tensorflow 2.11.0 requires tensorboard<2.12,>=2.11, but you have tensorboard 2.7.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed graphviz-0.8.4 matplotlib-3.5.1 mxnet-1.9.0 opencv-python-4.5.5.62 pydensecrf-1.0rc2 scikit-image-0.19.1 tensorboard-2.7.0 timm-0.4.12 torch-1.10.1 torchvision-0.11.2 tqdm-4.62.3\n","\u001b[0m\u001b[01;34mdeeplab\u001b[0m/           README.md         train_deeplab.py\n","evaluation.py      requirements.txt  train_TransCAM.py\n","example.png        \u001b[01;34mtblog\u001b[0m/            TransCAM.log\n","infer_aff.py       test_deeplab.py   TransCAM_PART_DATASET.log\n","infer_TransCAM.py  \u001b[01;34mtool\u001b[0m/             visualize.ipynb\n","\u001b[01;34mnetwork\u001b[0m/           train_aff.py      \u001b[01;34mvoc12\u001b[0m/\n"]}]},{"cell_type":"code","source":["import numpy as np\n","import torch\n","import os\n","\n","from torch import optim\n","from torch.utils.data import DataLoader\n","from torchvision import transforms\n","import voc12.data\n","from tool import pyutils, imutils\n","import argparse\n","import importlib\n","##from torch.utils.tensorboard import SummaryWriter\n","import torch.nn.functional as F\n","\n","from torch.utils.data import Dataset\n","import PIL.Image\n","import os.path\n","import scipy.misc\n","\n","from glob import glob"],"metadata":{"id":"4LqEVoAkrRho"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Patch to images\n","train_folder = '../dataset/images/train/'\n","print('Number of files in the train folder', len(os.listdir(train_folder)))"],"metadata":{"id":"5zJHctLCbXD4","executionInfo":{"status":"ok","timestamp":1677324454816,"user_tz":-180,"elapsed":72909,"user":{"displayName":"Simon EA45","userId":"04343123245495133347"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"24e594b6-41ee-48a6-ad80-1681127ad0e3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of files in the train folder 8549\n"]}]},{"cell_type":"code","source":["# TransCAM parameters\n","num_workers = 0;\n","lr = 2e-6;\n","wt_dec = 5e-4;\n","batch_size = 8;\n","max_epoches = 80;\n","network = \"network.conformer_CAM\";\n","arch = \"sm\";\n","\n","session_name = \"TransCAM_FULL_FULL_DATASET\"\n","crop_size = 512;\n","weights = '../Conformer_small_patch16.pth'\n","tblog_dir ='./tblog'\n","save_dir ='./';\n"],"metadata":{"id":"TVFFFHmQoAOG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\" Functions \"\"\"\n","reg = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=np.float32);\n","def load_image_label_list_from_npy(img_name_list, img_class_dict):\n","  cls_labels_dict = {};\n","  \n","  for img_name in img_name_list:\n","    cls_labels_dict[img_name] = reg.copy();\n","    for class_number in list(img_class_dict[img_name]):\n","      cls_labels_dict[img_name][class_number] = 1.0;\n","    \n","  return [cls_labels_dict[img_name] for img_name in img_name_list]\n","\n","IMG_FOLDER_NAME = train_folder;\n","def get_img_path(img_name):\n","    return os.path.join(IMG_FOLDER_NAME, img_name + '.jpg')\n","\n","def load_img_name_list(dataset_path):\n","  img_gt_name_list = sorted(glob(os.path.join(dataset_path, \"*.jpg\")));\n","  img_name_list = [img_gt_name.split('/')[-1].split('.')[0] for img_gt_name in img_gt_name_list];\n","  return img_name_list\n","\n","\n","\"\"\" Classes \"\"\"\n","class VOC12ImageDataset(Dataset):\n","\n","    def __init__(self, img_name_list_path, img_class_dict, transform=None):\n","        self.img_name_list = load_img_name_list(img_name_list_path) # Image names without .jpg\n","        self.img_class_dict = img_class_dict;\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.img_name_list)\n","\n","    def __getitem__(self, idx):\n","        name = self.img_name_list[idx]\n","\n","        img = PIL.Image.open(get_img_path(name)).convert(\"RGB\") # Open image\n","\n","        if self.transform:\n","            img = self.transform(img)\n","\n","        return name, img\n","        \n","class VOC12ClsDataset(VOC12ImageDataset):\n","\n","    def __init__(self, img_name_list_path, img_class_dict, transform=None):\n","        super().__init__(img_name_list_path, img_class_dict, transform)\n","        self.label_list = load_image_label_list_from_npy(self.img_name_list, self.img_class_dict) # Massive of the class vectors for each image\n"," \n","    def __getitem__(self, idx):\n","        name, img = super().__getitem__(idx)\n","\n","        label = torch.from_numpy(self.label_list[idx])\n","\n","        return name, img, label # Image name; Image; Class vector\n","\n"],"metadata":{"id":"DTiehbDrmPXo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Read labels from csv and transform it to the transCam format\n","from tqdm import tqdm\n","import csv\n","\n","img_names = load_img_name_list(train_folder);\n","\n","# Read box of each image\n","with open('../bbox_full_train.csv', 'r') as csvfile:\n","\n","  # creating a csv reader object\n","  csvreader = csv.reader(csvfile)\n","\n","\n","  # extracting each data row one by one\n","  row_namez = [];\n","  row_class = [];\n","  for row in tqdm(csvreader):\n","    row_namez.append(row[0]);\n","    row_class.append(row[1]);\n","\n","  # Extract class indexes\n","  img_class_dict = {};\n","  \n","  for img_name in img_names:\n","    counter=0;\n","    class_index = set();\n","    for row_name in row_namez:\n","      if(img_name == row_name):\n","        class_index.add( int(row_class[counter]) );\n","      else:\n","        None;\n","      counter = counter+1;\n","\n","    img_class_dict[img_name] = class_index;\n","\n","print( len(img_class_dict) )"],"metadata":{"id":"LnlsGVPRig8x","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1677324455533,"user_tz":-180,"elapsed":724,"user":{"displayName":"Simon EA45","userId":"04343123245495133347"}},"outputId":"b7de086c-cbb2-4480-ecb7-e2daaa54a92b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["8559it [00:00, 10030.80it/s]\n"]}]},{"cell_type":"code","source":["\n","pyutils.Logger(session_name + '.log')\n","\n","model = getattr(importlib.import_module(network), 'Net_' + arch)()\n","\n","# tblogger = SummaryWriter(tblog_dir)\n","\n","train_dataset = VOC12ClsDataset(train_folder, img_class_dict,\n","                                            transform=transforms.Compose([\n","                                                imutils.RandomResizeLong(320, 640),\n","                                                transforms.RandomHorizontalFlip(),\n","                                                transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3,\n","                                                                      hue=0.1),\n","                                                np.asarray,\n","                                                imutils.Normalize(),\n","                                                imutils.RandomCrop(crop_size),\n","                                                imutils.HWC_to_CHW,\n","                                                torch.from_numpy\n","                                            ]))\n","\n","train_data_loader = DataLoader(train_dataset, batch_size=batch_size,\n","                                shuffle=True, num_workers=0, pin_memory=True, drop_last=True)\n","\n","optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=wt_dec, eps=1e-8)\n","\n","checkpoint = torch.load(weights, map_location='cpu')\n","if 'model' in checkpoint.keys():\n","    checkpoint = checkpoint['model']\n","else:\n","    checkpoint = checkpoint\n","model_dict = model.state_dict()\n","for k in ['trans_cls_head.weight', 'trans_cls_head.bias']:\n","    print(f\"Removing key {k} from pretrained checkpoint\")\n","    del checkpoint[k]\n","for k in ['conv_cls_head.weight', 'conv_cls_head.bias']:\n","    print(f\"Removing key {k} from pretrained checkpoint\")\n","    del checkpoint[k]\n","pretrained_dict = {k: v for k, v in checkpoint.items() if k in model_dict}\n","model_dict.update(pretrained_dict)\n","model.load_state_dict(model_dict)\n","\n","model = torch.nn.DataParallel(model).cuda()\n","model.train()\n","\n","avg_meter = pyutils.AverageMeter('loss')\n","\n","TRAIN_LOSS = [];\n","for ep in tqdm(range(max_epoches)):\n","    print('ep num', ep)\n","    for iter, pack in enumerate(train_data_loader):\n","        _, img, label = pack\n","        N, C, H, W = img.size()\n","        bg_score = torch.ones((N, 1))\n","        label = torch.cat((bg_score, label), dim=1)\n","        label = label.cuda(non_blocking=True).unsqueeze(2).unsqueeze(3)\n","\n","        logits_conv, logits_trans, cams = model('transcam', img)\n","\n","        loss = F.multilabel_soft_margin_loss((logits_conv + logits_trans).unsqueeze(2).unsqueeze(3)[:, 1:, :, :], label[:, 1:, :, :])\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","    TRAIN_LOSS.append(loss);\n","\n","torch.save(model.module.state_dict(), os.path.join(save_dir, session_name + '.pth'))"],"metadata":{"id":"fOSg827IoH_I","colab":{"base_uri":"https://localhost:8080/"},"outputId":"c7302653-c7c4-4737-a05b-5e917326527a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Removing key trans_cls_head.weight from pretrained checkpoint\n","Removing key trans_cls_head.bias from pretrained checkpoint\n","Removing key conv_cls_head.weight from pretrained checkpoint\n","Removing key conv_cls_head.bias from pretrained checkpoint\n","ep num 0\n","epoch:     0 loss: 0.1159\n","ep num 1\n","epoch:     1 loss: 0.0980\n","ep num 2\n","epoch:     2 loss: 0.0971\n","ep num 3\n","epoch:     3 loss: 0.0928\n","ep num 4\n","epoch:     4 loss: 0.0912\n","ep num 5\n","epoch:     5 loss: 0.0890\n","ep num 6\n","epoch:     6 loss: 0.0859\n","ep num 7\n"]}]},{"cell_type":"code","source":["from matplotlib import pyplot as plt\n","\n","train_loss_numpy = [];\n","for i in TRAIN_LOSS:\n","  train_loss_numpy.append( i.cpu().detach().numpy() )\n","\n","plt.figure()\n","plt.plot(train_loss_numpy)\n","plt.xlabel('epoch')\n","plt.ylabel('loss')\n","plt.show()"],"metadata":{"id":"m1lmPnYtoIqY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Check inference on the test dataset\n","\n","# Model parameters\n","import importlib\n","from torchvision import transforms\n","import torch.nn.functional as F\n","\n","model = getattr(importlib.import_module('network.conformer_CAM'), 'Net_sm')()\n","model.load_state_dict(torch.load('../TransCAM_FULL_FULL_DATASET.pth'))\n","model = model.cpu()\n","model.eval()\n","\n","transform = transforms.Compose([\n","    transforms.Resize((640, 640)),\n","    np.asarray,\n","    imutils.Normalize(),\n","    imutils.HWC_to_CHW,\n","    torch.from_numpy\n","])\n","\n","\n","voc12_root = '../dataset/images/test/'\n","\n","def load_img_name_list(dataset_path):\n","  img_gt_name_list = sorted(glob(os.path.join(dataset_path, \"*.jpg\")));\n","  img_name_list = [img_gt_name.split('/')[-1].split('.')[0] for img_gt_name in img_gt_name_list];\n","  return img_name_list\n","\n","def get_img_path(img_name):\n","    return os.path.join(voc12_root, img_name + '.jpg')\n","\n","def load_img_name_list(dataset_path):\n","  img_gt_name_list = sorted(glob(os.path.join(dataset_path, \"*.jpg\")));\n","  img_name_list = [img_gt_name.split('/')[-1].split('.')[0] for img_gt_name in img_gt_name_list];\n","  return img_name_list\n","\n","\n","def visualize_img(name, label):\n","    img_path = get_img_path(name)\n","    orig_img = np.asarray(Image.open(img_path))\n","    orig_img_size = orig_img.shape[:2]\n","    img = transform(Image.open(img_path))\n","    _, _, cams = model('transcam', img.unsqueeze(0).cpu())\n","    cams = F.interpolate(cams, orig_img_size, mode='bilinear', align_corners=False).detach()\n","    cams = cams.cpu().numpy()[0][1:]\n","    cams[cams < 0] = 0\n","    cam_max = np.max(cams, (1, 2), keepdims=True)\n","    cam_min = np.min(cams, (1, 2), keepdims=True)\n","    norm_cam = (cams - cam_min) / (cam_max - cam_min + 1e-5)\n","    cam = norm_cam[label]\n","    print(np.max(cam))\n","    pseudo_mask = (np.sum(norm_cam, axis=0)>0.51);\n","    visualize(cam, orig_img)\n","\n","def visualize(normalized_heatmap, original=None):\n","    map_img = np.uint8(normalized_heatmap * 255)\n","    heatmap_img = cv2.applyColorMap(map_img, cv2.COLORMAP_JET)\n","    if original is not None:\n","        original_img = cv2.cvtColor(original, cv2.COLOR_RGB2BGR)\n","        img = cv2.addWeighted(heatmap_img, .8, original_img, 0.5, 0)\n","    else:\n","        img = heatmap_img\n","    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","    super_img = np.hstack((img, original))\n","    fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(10, 10))\n","    axes.imshow(super_img)\n","    plt.show()\n"],"metadata":{"id":"FHDKYFXB2zQj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from PIL import Image\n","import cv2\n","# {'Cassava Leaf Disease': 0,\n","# 'Corn or Maize': 1, \n","# 'Fruit Plants': 2, \n","# 'Herbarium': 3, \n","# 'Plant Pathology': 4, \n","# 'Tomato detection': 5, \n","# 'Wild Edible Plants': 6, \n","# 'flower_classification': 7}\n","\n","img_names = load_img_name_list(voc12_root);\n","img_name_1 = img_names[3] #130 124\n","img_label_1 = 3\n","visualize_img(img_name_1, img_label_1)\n","\n","# img_name_1 = 'aac893a91'\n","# img_label_1 = 2\n","# visualize_img(3, img_names[130])\n"],"metadata":{"id":"VTuzMtZy7vAD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"vf8WvAU1oA8x"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"xb7R61SqBuy-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"oe4dMBZ2B55e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"udvZKoDhp10R"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"k4EYnl9wp2V0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"p7mQYzSup2yc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Snj_FKwsDJHW"},"execution_count":null,"outputs":[]}]}